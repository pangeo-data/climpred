{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PredictionEnsemble Objects\n",
    "\n",
    "One of the major features of `climpred` is our objects that are based upon the `PredictionEnsemble` class. We supply users with a `HindcastEnsemble` and `PerfectModelEnsemble` object. We encourage users to take advantage of these high-level objects, which wrap all of our core functions. These objects don't comprehensively cover all functions yet, but eventually we'll deprecate direct access to the function calls in favor of the lightweight objects.\n",
    "\n",
    "Briefly, we consider a `HindcastEnsemble` to be one that is initialized from some observational-like product (e.g., assimilated data, reanalysis products, or a model reconstruction). Thus, this object is built around comparing the initialized ensemble to various observational products. In contrast, a `PerfectModelEnsemble` is one that is initialized off of a model control simulation. These forecasting systems are not meant to be compared directly to real-world observations. Instead, they provide a contained model environment with which to theoretically study the limits of predictability. You can read more about the terminology used in `climpred` [here](terminology.html).\n",
    "\n",
    "Let's create a demo object to explore some of the functionality and why they are much smoother to use than direct function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "from climpred import HindcastEnsemble\n",
    "import climpred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pull in some sample data that is packaged with `climpred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climpred.tutorial.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HindcastEnsemble\n",
    "\n",
    "We'll start out with a `HindcastEnsemble` demo, followed by a `PerfectModelEnsemble` case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind = climpred.tutorial.load_dataset('CESM-DP-SST') # CESM-DPLE hindcast ensemble output.\n",
    "obs = climpred.tutorial.load_dataset('ERSST') # ERSST observations.\n",
    "recon = climpred.tutorial.load_dataset('FOSI-SST') # Reconstruction simulation that initialized CESM-DPLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CESM-DPLE was drift-corrected prior to uploading the output, so we just need to subtract the climatology over the same period for our other products before building the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = obs - obs.sel(time=slice(1964, 2014)).mean('time')\n",
    "recon = recon - recon.sel(time=slice(1964, 2014)).mean('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate the `HindcastEnsemble` object and append all of our products to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindcast = HindcastEnsemble(hind) # Instantiate object by passing in our initialized ensemble.\n",
    "print(hindcast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just use the `add_` methods to attach other objects. See the API [here](api.html#add-and-retrieve-datasets). **Note that we strive to make our conventions follow those of `xarray`'s**. For example, we don't allow inplace operations. One has to run `hindcast = hindcast.add_reference(...)` to modify the object upon later calls rather than just `hindcast.add_reference(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindcast = hindcast.add_reference(recon, 'reconstruction')\n",
    "hindcast = hindcast.add_reference(obs, 'ERSST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hindcast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply most standard `xarray` functions directly to our objects! `climpred` will loop through the objects and apply the function to all applicable `xarray.Datasets` within the object. If you reference a dimension that doesn't exist for the given `xarray.Dataset`, it will ignore it. This is useful, since the initialized ensemble is expected to have dimension `init`, while other products have dimension `time` (see more [here](setting-up-data.html)).\n",
    "\n",
    "Let's start by taking the ensemble mean of the initialized ensemble so our metric computations don't have to take the extra time on that later. I'm just going to use deterministic metrics here, so we don't need the individual ensemble members. Note that above our initialized ensemble had a `member` dimension, and now it is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindcast = hindcast.mean('member')\n",
    "print(hindcast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have a trend in all of our products, so we could also detrend them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindcast.get_reference('reconstruction').SST.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import detrend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to transpose this first since my initialized ensemble has dimensions ordered `(init, lead)` and `scipy.signal.detrend` is applied over the last axis. I'd like to detrend over the `init` dimension rather than `lead` dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindcast = hindcast.transpose().apply(detrend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it looks like everything got detrended by a linear fit! That wasn't too hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindcast.get_reference('reconstruction').SST.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindcast.get_initialized().isel(lead=0).SST.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done our pre-processing, let's quickly compute some metrics. Check the metrics page [here](metrics.html) for all the keywords you can use. The [API](api.html#analysis-functions) is currently pretty simple for the `HindcastEnsemble`. You can essentially compute standard skill metrics and a reference persistence forecast.\n",
    "\n",
    "If you just pass a metric, it'll compute the skill metric against all references and return a dictionary with keys of the names the user entered when adding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindcast.compute_metric(metric='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also directly call individual references to compare to. Here we leverage `xarray`'s plotting method to compute Mean Absolute Error and the Anomaly Correlation Coefficient for both our reference products, as well as the equivalent metrics computed for persistence forecasts for each of those metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-talk')\n",
    "\n",
    "RECON_COLOR = '#1b9e77'\n",
    "OBS_COLOR = '#7570b3'\n",
    "\n",
    "f, axs = plt.subplots(nrows=2, figsize=(8, 8), sharex=True)\n",
    "\n",
    "for ax, metric in zip(axs.ravel(), ['mae', 'acc']):\n",
    "    handles = []\n",
    "    for product, color in zip(['reconstruction', 'ERSST'], [RECON_COLOR, OBS_COLOR]):\n",
    "        p1, = hindcast.compute_metric(product, metric=metric).SST.plot(ax=ax, \n",
    "                                                                 marker='o', \n",
    "                                                                 color=color,\n",
    "                                                                 label=product,\n",
    "                                                                 linewidth=2)\n",
    "        p2, = hindcast.compute_persistence(product, metric=metric).SST.plot(ax=ax, \n",
    "                                                                      color=color, \n",
    "                                                                      linestyle='--',\n",
    "                                                                      label=product + ' persistence')\n",
    "        handles.append(p1)\n",
    "        handles.append(p2)\n",
    "    ax.set_title(metric.upper())\n",
    "    \n",
    "\n",
    "axs[0].set_ylabel('Mean Error [degC]')\n",
    "axs[1].set_ylabel('Correlation Coefficient')\n",
    "axs[0].set_xlabel('')\n",
    "axs[1].set_xlabel('Lead Year')\n",
    "axs[1].set_xticks(np.arange(10)+1)\n",
    "\n",
    "# matplotlib/xarray returning weirdness for the legend handles.\n",
    "handles = [i.get_label() for i in handles]\n",
    "\n",
    "# a little trick to put the legend on the outside.\n",
    "plt.legend(handles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n",
    "\n",
    "plt.suptitle('CESM Decadal Prediction Large Ensemble Global SSTs', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PerfectModelEnsemble\n",
    "\n",
    "We'll now play around a bit with the `PerfectModelEnsemble` object, using sample data from the MPI perfect model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climpred import PerfectModelEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = climpred.tutorial.load_dataset('MPI-PM-DP-1D') # initialized ensemble from MPI\n",
    "control = climpred.tutorial.load_dataset('MPI-control-1D') # base control run that initialized it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = climpred.PerfectModelEnsemble(ds)\n",
    "pm = pm.add_control(control)\n",
    "print(pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objects are carrying sea surface temperature (`tos`), sea surface salinity (`sos`), and the Atlantic Multidecadal Oscillation index (`AMO`). Say we just want to look at skill metrics for temperature and salinity over the North Atlantic in JJA. We can just call a few easy `xarray` commands to filter down our object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = pm.drop('AMO').sel(area='North_Atlantic', period='JJA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily compute for a host of metrics. Here I just show a number of deterministic skill metrics comparing all individual members to the initialized ensemble mean. See [comparisons](comparisons.html) for more information on the `comparison` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = ['mse', 'rmse', 'mae', 'acc',\n",
    "           'nmse', 'nrmse', 'nmae', 'msss']\n",
    "\n",
    "result = []\n",
    "for metric in METRICS:\n",
    "    result.append(pm.compute_metric(metric, comparison='m2e'))\n",
    "    \n",
    "result = xr.concat(result, 'metric')\n",
    "result['metric'] = METRICS\n",
    "\n",
    "# Leverage the `xarray` plotting wrapper to plot all results at once.\n",
    "result.to_array().plot(col='metric', \n",
    "                       hue='variable', \n",
    "                       col_wrap=4, \n",
    "                       sharey=False, \n",
    "                       sharex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to compare the initialized ensemble to an uninitialized run. See [terminology](terminology.html) for a description on \"uninitialized\" simulations. This gives us information about how *initializations* lead to enhanced predictability over knowledge of external forcing, whereas a comparison to persistence just tells us how well a dynamical forecast simulation does in comparison to a naive method. We can use the `generate_uninitialized()` method to bootstrap the control run and create a pseudo-ensemble that approximates what an uninitialized ensemble would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = pm.generate_uninitialized()\n",
    "print(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = pm.drop('tos') # Just assess for salinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I plot the ACC for the initialized, uninitialized, and persistence forecasts for North Atlantic sea surface salinity in JJA. I add circles to the lines if the correlations are statistically significant for $p <= 0.05$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACC for initialized ensemble\n",
    "acc = pm.compute_metric('acc')\n",
    "acc.sos.plot(color='red')\n",
    "acc.where(pm.compute_metric('pval') <= 0.05).sos.plot(marker='o', linestyle='None', color='red', label='initialized')\n",
    "\n",
    "# ACC for 'uninitialized' ensemble\n",
    "acc = pm.compute_uninitialized('acc')\n",
    "acc.sos.plot(color='gray')\n",
    "acc.where(pm.compute_uninitialized('pval') <= 0.05).sos.plot(marker='o', linestyle='None', color='gray', label='uninitialized')\n",
    "\n",
    "# ACC for persistence forecast\n",
    "acc = pm.compute_persistence('acc')\n",
    "acc.sos.plot(color='k', linestyle='--')\n",
    "acc.where(pm.compute_persistence('pval') <= 0.05).sos.plot(marker='o', linestyle='None', color='k', label='persistence')\n",
    "\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
